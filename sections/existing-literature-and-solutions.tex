\section{Existing literature and solutions} % What is already known and done + existing solutions?
%\todo{Det her er det samme som "Background" - m√•ske}

%%% Some intro to this maybe... /// studies and
In this section we will describe different findings from existing literature in order to provide insights from the field of classroom response systems. 
%These insights will help us understand how previous work has been done and how we can evaluate our solution. 
Furthermore, we will include a description of significant existing solutions in order to compare their features. %This comparison will tell us what's missing from existing solutions and which problems we will address.
The following section will include these findings, results and mentions of some of the most significant research in the field of response systems in classrooms. %Furthermore, this section will mention research based response systems developed at MIT and University of Lugana.

\subsection{Literature and research}

Some of the least recent literature dating back to around 2007, is mainly focused on physical clicker devices, where the students buy actual "remote controls" for polling, in a time where more modern solutions (including the smartphone) has not yet matured. Even today, the physical clicker devices still seem to play a significant role in the \emph{"clicker community"}. \citeA{stowell2015use} explains how the use of clicker systems on physical devices has a greater response rate compared to mobile, but with no significant differences in the final grades of the students. Also, the introduction of students bringing their own devices (BYOD) introduces new issues such as loss of internet connectivity and the risk of being distracted by the device itself (pp. 331-332).

One of the main reasons to use a CRS is the importance of interactivity in learning \cite{draper2004increasing}. Studies find that CRS engages interactivity, helps students stay active during lectures \cite[p.~116]{moredich2007engaging}, improves learning \cite<e.g.,>{siau2006use, yourstone2008classroom} and increases attention \cite[pp.~86-88]{sun2014influence}. While \citeA{yourstone2008classroom} focuses on actual results in learning outcomes and \citeA{sun2014influence} uses brainwave analysis to determine level of attention while using CRS, most other studies take on a qualitative approach and evaluate teachers and students by asking to their engagement, participation and learning outcomes \cite<e.g.,>{moredich2007engaging}. The latter approach seems to be the most widespread in the literature and focuses more on whether or not students feel an increased engagement and participation rather than their actual performance and examination results. 

\citeA{stowell2007benefits} takes on the qualitative approach where they ask into students emotions in a introductory psychology class before, during and after lecture. The study compares standard lectures to hand raising, response card and "clicker" lectures\footnote{Standard lectures are thought of as normal lectures where the teacher asks open ended questions to the students. Hand raising is the approach where students are asked to raise their hand if they agree to a statement. Response cards are cards labeled A, B, C, D and are sort of the same as hand raising \cite[p.~254]{stowell2007benefits}.}.
They find that more correct answers to questions are present in response card lectures but explains the much higher correctness with the ability to see what peers are answering \cite[p.~257]{stowell2007benefits}. The same applies for hand raising. Furthermore the study finds that there's a small positive impact on emotions towards the class depending on which kind of lecture is taught (i.e. standard, hand raising, response card and "clicker") \cite{stowell2007benefits}.

One study found that there wasn't a framework for measuring interactivity in classrooms \cite[p.~400]{siau2006use}. They then designed a test framework which consists of a \emph{pretest}, \emph{implementation}, \emph{posttest} and some \emph{qualitative data collection}. The pretest is done before starting using a classroom response system. The posttest asks the same questions as the pretest which are then compared to measure difference in interactivity. The posttest, which is done at the end of the course, asks further into the technology and the response system used in the classroom and how it's perceived \cite{siau2006use}. The implementation is simply the implementation of the response system in the class which are done after the pretest. Finally, the qualitative data collection asks into advantages and disadvantages of using a response system. Their study found a significantly increased interaction reported by the students who participated, where the increase was measured on an individual and general level \cite[p.~400]{siau2006use}.


Among the biggest studies of CRSs is \emph{Increasing interactivity in lectures using an electronic voting system} by \citeA{draper2004increasing}. This particular study took place over a two year period and found (among other things) that the benefits increases as teachers became aware how to exploit the pedagogy behind the system \cite[p.~93]{draper2004increasing}. The study took place in a time where there was a need for physical hardware (e.g., remote controllers and receivers) in order to facilitate a CRS. Their study addresses the need to learn how to proper use a CRS in order to get the benefits from it.

The next parts will focus on some articles which describes and study some different kind of response systems. The studies take on different approaches at teaching computer science and the response systems include atypical features.

All of the before mentioned studies have either not specified which system was used or they used one of the traditional systems available such as the iClicker. Another type of CRS called \emph{Informa} focuses mainly on teaching Java \cite{Hauswirth09}. The system is a software based solution made in Java. The system consists of two different clients; an instructor client and several student clients \cite[p.~1]{Hauswirth09}. Informa is different because it's not a traditional response system, but a task based peer-grading system where the instructor creates a task which the students then are supposed to solve. Students who finish early can grade other students solutions while waiting for everyone to finish \cite{Hauswirth09}. One of \citeA[p.~9]{Hauswirth09}'s findings are that students find the system useful but it's unsure if the system is more useful for the weaker or stronger students. \emph{Informa} is worth mentioning because it is not a typical response system and because it bridges the gap between classroom response systems and teaching a programming language. However, it seems to be limited to teaching Java and cannot be used to teach different programming languages.

A similar approach can be found in the pen-and-tablet based system that has been used to teach introductory computer science at MIT \cite{koile2007supporting}. The system is called \emph{Classroom Learning Partner} and is a response system where the students answer to question by writing with a pen on a tablet. The hand-written answer is recognized and interpreted by the system, but not without difficulties \cite[pp.~2-3]{koile2007supporting}. A previous study found that their system could improve learning in a CS1 (Computer Science 1) class \cite{koile2006improving}. The study introduced one of two classes to the tablet based system after the first exam (in week 5 of 15) \cite[pp.~6-7]{koile2006improving}. The class not using the system functioned as control group \cite[pp.~6-7]{koile2006improving}. By comparing the score of the students in the different classes their preliminary results showed a difference between the two, where the tablet-PC class performed slightly better \cite{koile2006improving}. They also report that fewer than expected students performed poorly in the tablet-PC class, where \emph{"23.5\% of non-Tablet-PC students were in the bottom 25\% of the class, vs. 6.0\%."} \cite[p.~7]{koile2006improving}. % The significance was p < .05.

The above shows that there is not any one size fits all system, and that there is no conclusive or optimal approach to CRSs. Different approaches has been taken towards improving learning via CRSs in different ways. Each system has it's own strengths and weaknesses, some are very expensive, some are cheap, and some are even free, as we will see in the following section. Some are feature-rich and some are only able to respond with a simple letter for an answer.

%\todo{Det her skal rundes af med en kort opsummering af overordnede pointer}