\section{Discussion}



%in the following we will interpret and describe the significance of our findings. in light of what was already known about the research problem being investigated


CRSFIT has proven to be found close to equally useful compared to one of the more well established systems, Mentimeter. Even though some of the compared statements values were lower in CRSFIT, the patterns discovered is the same (see figure \ref{fig:perceived_ease_of_use_and_usefulness}). Based on the results the system works as intended, and the varying results in the comparison has been explained in the sections above. Undoubtedly some parts of the setup and test has been biased by the way it was carried out, and the results might have suffered from this.


\subsection{Improving the setup and test}
Given the time we would have done our testing differently. In order to tell if our system could increase interactivity in courses where programming or mathematics are taught, we should have done further and different testing that could improve the accuracy of the results. A single test would normally be done during a whole semester, and not only during one lecture as it was done in this project. The pretest should be done in the beginning or a few weeks into the course to establish base results which the implementation of the CRS should aim to improve. The posttest should be done in the ending of the semester and it should ask the same questions concerning individual and general questions as the pretest did. By comparing the pretest and posttest we would be able to tell if the students participating in the test feel an increase in their own and general interactivity. It's important to notice that any increasements measured can't be seen as a direct effect of implementing a CRS but only that it didn't decrease the interactivity. 

Since the test was performed over the duration of a single lecture in both courses we had to do the pretest, implementation and posttest during this time. It wouldn't make sense to ask the students in the Frameworks course to answer the same questions in the pre- and posttest, since they would only have had roughly 30 minutes of exposure to the system. Based on this, any significant differences found would be less likely to be valid due to the short time they had to use the system. We did however find that when asked into whether or not students feel CRSFIT enhances their interactivity, only person answered negatively with a score below 5 towards two of three questions in \emph{perceived usefulness}. Furthermore, the setting was not fully optimal since it was not a normal lecture, as we carried out the \emph{implementation} part of the test asking 10 questions in a row. The optimal setting would be normal lectures where the teacher responsible for the course asked the students one or two questions related to the lecture's subject at suitable moments. The systems should ultimately be used over a longer period of time before we would be able to conclude if it could increase the interaction. The same counts for the test which was carried out in the Advanced course. The students were given the pretest questionnaire in the beginning of a lecture and answered the posttest questionnaire in the end of the lecture. Had we developed the questionnaires earlier in the process of this project we could have given students the pretest at an earlier point of the semester in order to establish a base result which we could aim to improve.

The amount of participants should optimally be higher. The small group of students from especially the Frameworks course who participated in the test is not sure to be representative for all students enrolled in the course. We should therefore aim at having a larger amount of all the students from the courses participating in the tests.



Some questions in the pretest got a large amount of neutral responses (5 on the scale) as mentioned in the analysis of the test. In particular the last two questions in the pretest (appendix \ref{app:pretest-general}, item 8 and 9). When we offer respondents to answer 1-9 on a Likert scale we don't take into consideration if the respondents don't have an opinion or don't know how they would assess the statement. Therefore, we should consider providing an "I don't know"-option together with the scale in a future test. 


\subsection{Code and math notation}
A future test should try to investigate more details about presenting code and math in CRSFIT. As of now, the only question in our test asking into any details about code is the one where we ask whether or not the students tested and ran code from the questions. This single question did however give us some idea of how students could easily copy and paste code from the questions in order to find the correct answer. While it's meant to be a challenge to figure out the correct answer, running the code and get the correct answer defeats the purpose of figuring it out yourself. Every question we asked with CRSFIT in the Frameworks course was able to be solved by simply testing it. Limiting the time given to answer such questions could prevent most questions from being run and tested. This would rely on how the person asking questions are using CRSFIT and can not be prevented by us. A future version of CRSFIT could have a feature for automatically closing a question after a desired period of time and the system could suggest using the feature if the question involved code. However, our results as of now does not show that writing code (or math) within a CRS is beneficial. This needs further research.


If we were to investigate more details about how code and math are presented and used in CRSFIT, we could add questions to the test asking into how students find the presentation of code and math and talk to teachers how they find writing code and math in the system. Writing code in a question or answer can be a challenge as of how the system is right now. %Since we are using a ...

Seen from a competitive standpoint, CRSFIT is highly dependent on the success of the source code and math notation features. The rivalry among other CRSs is high, and substituting or shifting systems from semester to semester is easy unless more critical features like grading, are build into it. Also CRSFIT is a proof of concept product. It has less features than many of the other products available, and it's primary focus is on improving interaction in IT classes. The system is also not production ready. Minor usability bugs when entering code formatted questions in the WYSIWYG editor was found, and would have to be fixed before opening for public use. In fact, using the WYSIWYG editor has proven to over complicate things, and should probably be removed. Not many will discover this problem as it's only visible when entering questions and answers into the system which includes code.


During the development of CRSFIT we have neglected usability of the system. When comparing CRSFIT to any other system mentioned in this project, we have only chosen to compare specific features and functions. The list of features is not exhaustive, and is probably biased by the fact, that it is created by us. Another approach could have been to extensively research what features could be relevant through interviews with students and teachers and create the list based on that.

The system's usability and design has also been neglected to some extend since it is not relevant for the outcome of the research question at hand. Seen from a competitive standpoint our system could fail at gaining traction, if the usability is hopeless, since a well designed and well implemented system (like Mentimeter) could prove to be a better solution overall, despite lacking IT specific features. While comparing CRSFIT to other CRSs (in table \ref{tab:overview-2}), it becomes clear that CRSFIT doesn't support all the same features as some of the other CRSs mentioned. While some may use the ability of creating different question types in one system, CRSFIT only supports multiple choice questions. 





%Punkter til discussion:

%- Vores reelle konkurrencedygtighed (Porter)
%    - Prototype kontra færdigt produkt 
%- Undersøg dybere med kode/math
%- We've created an instrument for measuring our system
%    - Which we've tested

%- Vi undersøger ikke kode/math-delen som vi måske gerne ville have gjort for at sige noget om dét.
%    - "Did you find it easy to answer questions which include code?"
%    - "Using syntax highlighting helped me better understand the question asked"
%    - "I found it engaging answering questions based on code"
%        - Det kunne måske have været brugt til at sammenligne noget.

