\section{Discussion}



%in the following we will interpret and describe the significance of our findings. in light of what was already known about the research problem being investigated


CRSFIT seems almost as useful when compared to one of the more well established systems, Mentimeter. Even though some of the compared statements scored lower while testing CRSFIT, the patterns discovered was the same (see figure \ref{fig:perceived_ease_of_use_and_usefulness}).

%The varying results in the comparison has been described in depth, and possible reasons for it has been explained. 
%Based on the results the system has proven to work to some degree, and the varying results in the comparison has been explained in the analysis. 
%\todo{Ved ikke helt her}

Some parts of the setup and test might have been biased by the way it was carried out, and the results could have suffered from this. In this section we will discuss the results from the tests and how they can be seen in relation to previous literature and research. Furthermore, we will discuss how we could improve our test. \\


%\subsection{Results from our test}

A general observation from the test, is that we see more answers in the posttest compared to the pretest in both courses. The pretest is substantially longer than the posttest, which could be a reason for the varying amount of respondents in the first compared to the latter. In the Advanced course, the pretest was carried out at the very beginning of the class, so people showing up late could be a possible reason. In the Frameworks course, the test was carried out after one hour of lecture.


In our results we found that the students assess \emph{individual interactivity} lower than \emph{general interactivity} (section \ref{sec:individual_and_general}). A similar measurement was done by \citeA[p.~400]{siau2006use} where they found $6,1$ and $6,3$ based on the same scale respectively for individual and general interactivity. Our observation does not prove that there will always be a tendency to rate individual interactivity lower than general interactivity, and the low difference between individual and general interactivity for the Frameworks course should be an indicator of this. An explanation of the difference between the two averages could be, that when asked to assess the general interactivity every individual compare their own interactivity with the ones of a few people who always interact during lectures. Another explanation could be that the students participating in our test find themselves interacting less than the average of the entire class they are in.




%\ref{sec:individual_and_general}
%The biggest difference is found in the questions \emph{I participate in class discussion} and \emph{I provide my opinion to questions from the teacher during the class}. These questions are very specific to taking part in class and actively participating, where other questions like \emph{I am involved in learning during class}, might not necessarily imply any direct focus on the individual student. 
The results showed noticeable differences between the statements \emph{I participate in class discussion} and \emph{I provide my opinion to questions from the teacher during the class} (section \ref{sec:individual_and_general}). Reasons for these variations could be found in the differences between the two courses. Advanced Programming has formal prerequisites where you must be able to program, know basic functional programming, discrete mathematics and more. The Frameworks course is an introductory course so it has no formal prerequisites. Based on this, the learning curve could be steeper in the Advanced Programming course since it is considered an advanced curriculum, and therefore less people might participate in class discussions since the subjects discussed might be harder to comprehend at first. Also course size might have influenced the outcome. The Advanced course has more than twice as many students enrolled as the Frameworks course (89 vs. 42), here students might hold back their participation due to the large amount of people or there simply may not be enough time to have all students participate.

%%% I interact with the teacher in class
%The responses to the question \emph{I interact with the teacher in class} also has a large difference with a mean of $6$ in the Frameworks course versus $4,29$ in the Advanced course.

In the statement \emph{I interact with the teacher in class} we found a large difference with a mean of $6$ in the Frameworks course versus $4,29$ in the Advanced course. This outcome can most likely be explained by the larger class and tougher topics in the Advanced course as explained above. It might also involve the teachers style, where some might include students more, and some less. A point to notice though, is the fact, that the scores are on each side of the neutral value 5. Where $4,29$ is slightly more towards disagreeing and $6$ is the other way around.



%%% 
The large amount of neutral responses received in the questions \emph{Students can comprehend whether they are following the course materials during the class} and \emph{Students can assess their understanding of the course materials with respect to other students during the class} could suggest that they are statements which are difficult to assess. Another explanation could be that these statements are the very last in the pretest and students could be bored at this point and just answer 5 to whatever they are asked to assess. Or it could be that a large amount of the students have a neutral opinion to the statements. Either way, the results stands out from the rest with a relatively large amount of neutral answers.



%\ref{fig:perceived_ease_of_use_and_usefulness}  Ease of use and perceived usefulness
The lower level of perceived usefulness of CRSFIT compared to Mentimeter could be explained by the way the test was carried out. The results may have been different if the test was carried out over a longer period of time during regular lectures. For example, students might be more convinced that a CRS is useful if they have used it over the course of a whole semester, and therefore rate the perceived usefulness higher. This is compared to testing CRSFIT in just one lecture, where the system is new to them. As mentioned, the class using Mentimeter have had the chance to get well acquainted with the system before doing our test.


%%% Last two questions in pretest
The last two questions in the pretest from the Advanced course is surprisingly low considering they have used Mentimeter from the beginning of the semester. However, they may have responded relatively to the lecture in which the test was carried out. At the time, the class has been using a REPL (Read–Eval–Print Loop) for Scala, an interactive command line tool for easily running code, for several months, so a higher usage was expected. However, depending on the questions nature it might not be suitable for testing in the REPL, as it was the case in some of the questions in the Advanced course (for example appendix \ref{app:questions-advanced}, item 1).



%%% QUESITON 6
During the test in the Frameworks course, one person mentioned that question 6 (appendix \ref{app:questions}, item 6) was a math question. The question could be asked in any programming language and is not jQuery specific as many of the other questions. This particular question was designed to force the students to run the code since it otherwise would be hard to figure out the correct answer. At this point in the test, it was proposed that in order to answer the question, the students could simply run the code and get the answer. This could be an explanation why 60\% of the students from the Frameworks course answered that they ran the code from the questions on their computer. If it had not been brought up during the test, then fewer students might have answered that they ran the code. Despite being brought up how to answer question 6, one still failed to provide the correct answer.

Another important note on the difference between the two courses on whether or not they ran the code from the questions could be the availability of the code. Mentimeter was used by presenting the questions and answers in a separate slideshow and thus not entered into Mentimeter (because of the systems limitations). If these slideshows weren't available to the students during lectures, then copy-pasting the code into a REPL would not be an option and manually writing the code may be too much an effort in order to test the code from the questions. CRSFIT, on the other hand, provides the code and is easily copy-pasted.



%%% Qualitative questions

In the qualitative results, one student from the Frameworks course mentions that a disadvantage of (any) CRS could be that it takes time from helping the students. If the time using a CRS is taken from helping students individually, then it could be a disadvantage. However, it will often be the individual teacher's responsibility to manage the time in the courses they teach. Another student mentions awkward pauses when using Mentimeter (appendix \ref{app:qualitative-advanced}, item 13). This may however be the same for every CRS and depends on how the teacher uses the system. 




As highlighted by some of the respondents in the posttest from the Frameworks course, there's room for improvement when using CRSFIT. The way the test was carried out didn't resemble a regular lecture enough. As mentioned earlier in chapter \ref{sec:testingcrs}, teaching and using a CRS requires training and experience. The test of the system was not carried out in a regular lecture by a teacher as it would have been if it was a normal setting. The test was carried out by us, letting students answer one question at a time. Normally, a teacher might ask one or two questions after a period of time with lecturing in between or after finishing a subject, and not ten questions in a row as it was the case the test of CRSFIT.



%%% The last questino in the test...

%Two respondents from the Frameworks course answer that they would like to wait before seeing what other students answered. As one points out: \emph{"Students are easily biased, and it is hard to not look at what everyone else has answered before answering yourself."} (appendix \ref{app:qualitative-frameworks}, item 4). For most questions in the test, the majority answered the correct answer except for the very last question. This particular question was made harder and more comprehensive. See appendix \ref{app:questions} item 10. 

In the very last question asked with CRSFIT we may have seen biased answers because the students were able to see what their peers answered in real time. The question got 8 responses and only two of them were correct. This may be because one of the first received answers in CRSFIT was incorrect and most people just followed along due to the difficulty of the question. The students that stated they wanted to wait some time before revealing answers may have a point. If we waited some time before showing the responses for each question, we might have seen a larger spread in answers or maybe more correct answers in questions like the last one, because students would individually take the time to come up with an answer. 



%%% De-personalizes the connection...
It was pointed out that CRSFIT de-personalized the connection between teacher and student. A teacher may go more into detail whether one answer is correct and another isn't and then ask the students why an answer is (in)correct and use a CRS to start a discussion. This way the personal connection could be maintained for people who are used to interact in class while people not used to it would be included as well through the system. In the test of CRSFIT we briefly discussed some of the questions during the test, and did not go into depth about why an answer was correct or incorrect on most of the question. Only during the last question where a majority of the responses was incorrect, we discussed why.






\subsection{Future directions}
Given the time we would have done our testing differently. In order to tell if our system could increase interactivity in courses where programming or mathematics are taught, we should have done further and different testing that could improve the accuracy of the results. A single test would normally be done during a whole semester, and not only during one lecture as it was done in this project. The pretest should be done in the beginning or a few weeks into the course to establish base results which the implementation of the CRS should aim to improve. The posttest should be done in the ending of the semester and it should ask the same questions concerning individual and general questions as the pretest did. By comparing the pretest and posttest we would be able to tell if the students participating in the test feel an increase in their own and general interactivity. It's important to notice that any increasements measured can't be seen as a direct effect of implementing a CRS but only that it didn't decrease the interactivity.

Since the test was performed over the duration of a single lecture in both courses we had to do the pretest, implementation and posttest during this time. It wouldn't make sense to ask the students in the Frameworks course to answer the same questions in the pre- and posttest, since they would only have had roughly 30 minutes of exposure to the system. Based on this, any significant differences found would be less likely to be valid due to the short time they had to use the system. We did however find that when asked into whether or not students feel CRSFIT enhances their interactivity, only one person answered negatively with a score below 5 towards two out of three questions in \emph{perceived usefulness}. Furthermore, the setting was not fully optimal since it was not a normal lecture, as we carried out the \emph{implementation} part of the test asking 10 questions in a row. The optimal setting would be normal lectures where the teacher responsible for the course asked the students one or two questions related to the lecture's subject at suitable moments. The system should ultimately be used over a longer period of time before we would be able to determine if it could increase the interaction. This also applies for the test which was carried out in the Advanced course. The students were given the pretest questionnaire in the beginning of a lecture and answered the posttest questionnaire in the end of the lecture. Had we developed the questionnaires earlier in the process of this project we could have given students the pretest at an earlier point of the semester in order to establish a base result which we could aim to improve.

The amount of participants should optimally be higher. The small group of students from especially the Frameworks course who participated in the test is not sure to be representative for all students enrolled in the course. We should therefore aim at having a larger amount of all the students from the courses participating in the tests.

Some questions in the pretest got a large amount of neutral responses (5 on the scale) as mentioned in the results from the test. In particular the last two questions in the pretest (appendix \ref{app:pretest-general}, item 8 and 9). When we offer respondents to answer 1-9 on a Likert scale we don't take into consideration if the respondents do not have an opinion or do not know how they would assess the statement. Therefore, we should consider providing an "I don't know"-option together with the scale in a future test.


%\subsection{Code and math notation}
A future test should try to investigate more details about presenting code and math in CRSFIT. As of now, the only question in our test asking directly into any details about code is where we ask whether or not the students tested and ran code from the questions (see appendix \ref{app:posttest-open-ended}, item 2). 
%This single question did however give us some idea of how students can copy and paste code from the questions in order to find the correct answer. 




%While it's meant to be a challenge to figure out the correct answer, running the code and get the correct answer could defeat the purpose of figuring it out by yourself\todo{Er vi sikre på det?}. 

%Every question we asked with CRSFIT in the Frameworks course was able to be solved by testing it. Limiting the time given to answer such questions could prevent most questions from being run and tested. This would rely on how the person asking questions are using CRSFIT. A future version of CRSFIT could have a feature for automatically closing a question after a desired period of time and the system could suggest using the feature if the question involved code. However, our results as of now does not show that writing code (or math) within a CRS is beneficial. This needs further research.

If we were to investigate more details about how code and math are presented and used in CRSFIT, we could add questions to the test asking into this. We could then talk to teachers about how they find writing code and math into the system. Writing code in a question or answer can be a challenge as of now because of the WYSIWYG editor currently used in CRSFIT. 

Seen from a competitive standpoint, CRSFIT is highly dependent on the success of the source code and math notation features. The rivalry among other CRSs could prove to be high due to the many available solutions, and the switching cost from semester to semester is low unless more critical features like grading are build into it. Also, CRSFIT is a proof of concept product. It has less features than many of the other products available, and it's primary focus is on improving interaction in IT classes. The system is also not production ready. Minor usability bugs when pasting code in the WYSIWYG editor was found, and would have to be fixed before opening for general use. In fact, using the WYSIWYG editor has proven to over complicate things, and should probably be removed. %Depending on the usage, most people will probably not discover this problem as it's only visible when pasting questions and answers into the system which includes code.

During the development of CRSFIT we have to some extend neglected usability of the system. When comparing CRSFIT to any other system mentioned in this project, we have only chosen to compare specific features and functions. The list of features is not exhaustive, and is probably biased by the fact, that it is created by us. Another approach could have been to extensively research what features could be relevant through interviews with students and teachers and create the list based on that.

The system's usability and design has also been neglected to some extend since it is not relevant for the outcome of the research question at hand. Seen from a competitive standpoint our system could fail at gaining traction, if the usability is hopeless, since a well designed and well implemented system (like Mentimeter) could prove to be a better solution overall, despite lacking IT specific features. Comparing CRSFIT to other CRSs (in table \ref{tab:overview-2}), it becomes clear that CRSFIT doesn't support all the same features as some of the other CRSs mentioned. For example, CRSFIT only supports multiple choice questions while other systems supports a various amounts of different question types.
%For example while some may create different question types in one CRS, CRSFIT only supports multiple choice questions. 





%Punkter til discussion:

%- Vores reelle konkurrencedygtighed (Porter)
%    - Prototype kontra færdigt produkt 
%- Undersøg dybere med kode/math
%- We've created an instrument for measuring our system
%    - Which we've tested

%- Vi undersøger ikke kode/math-delen som vi måske gerne ville have gjort for at sige noget om dét.
%    - "Did you find it easy to answer questions which include code?"
%    - "Using syntax highlighting helped me better understand the question asked"
%    - "I found it engaging answering questions based on code"
%        - Det kunne måske have været brugt til at sammenligne noget.

